{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa2feb8",
   "metadata": {},
   "source": [
    "# Goodnight Moon, Hello Early Literacy Screening\n",
    "*https://www.drivendata.org/competitions/298/literacy-screening/*  \n",
    "*Solution for the 3rd place (0.2137)*  \n",
    "*Copyright (c) 2025 Igor Ivanov*  \n",
    "*Email: vecxoz@gmail.com*  \n",
    "*License: MIT*  \n",
    "*I will be happy to answer any questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa145d",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "A) Solution write-up  \n",
    "B) Results and conclusions  \n",
    "C) Installation  \n",
    "D) Full solution    \n",
    "E) Simplified solution (single model)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976fbd68",
   "metadata": {},
   "source": [
    "# A) Solution write-up\n",
    "\n",
    "### 1. Who are you (mini-bio) and what do you do professionally?\n",
    "\n",
    "My name is Igor Ivanov. I'm a deep learning engineer from Dnipro, Ukraine. I specialize in CV and NLP and work at a small local startup.\n",
    "\n",
    "### 2. What motivated you to compete in this challenge?\n",
    "\n",
    "In the first place it was an intriguing multimodal dataset allowing to extract different features. Also speech competitions are relatively rare, so it was a nice opportunity to apply current state-of-the-art models like Whisper.\n",
    "\n",
    "### 3. High level summary of your approach: what did you do and why?\n",
    "\n",
    "My approach is based on the PyTorch and Huggingface Transformers. I used 4 modalities of the data each of which brings significant improvement for the model:\n",
    "\n",
    "* original audio\n",
    "* audio generated from expected text (by SpeechT5)\n",
    "* original expected text\n",
    "* text transcribed from original audio (by Whisper finetuned on positive examples)\n",
    "\n",
    "To process these 4 components I built a multimodal classifier which uses the encoder from the finetuned Whisper model as audio feature extractor and Deberta-v3 as text feature extractor. The best single multimodal classifier is based on  `microsoft/deberta-v3-base` plus `openai/whisper-medium.en` or `openai/whisper-medium`. My best final submission is an ensemble of 6 multimodal classifiers each of which uses `microsoft/deberta-v3-base` and different versions of Whisper, namely:\n",
    "\n",
    "* `openai/whisper-medium.en`\n",
    "* `openai/whisper-medium`\n",
    "* `openai/whisper-small.en`\n",
    "* `distil-whisper/distil-medium.en`\n",
    "* `distil-whisper/distil-large-v3`\n",
    "* `distil-whisper/distil-large-v2`\n",
    "\n",
    "#### 3.1 Original audio\n",
    "\n",
    "I listened to several hundred training examples. The most complicated task by nature is nonword repetition. Also there are some very hard cases where the speech is unclear and blurry. Eventually I decided to use raw audio without any cleaning or preprocessing because Whisper was pretrained on a very large and versatile dataset.\n",
    "\n",
    "#### 3.2 Audio generated from expected text (Text-to-Speech)\n",
    "\n",
    "I generated speech from expected text using the SpeechT5 `microsoft/speecht5_tts` model with `microsoft/speecht5_hifigan` vocoder. As a speaker embedding I manually choose a vector from the `Matthijs/cmu-arctic-xvectors` dataset (index 7900). It is a female voice close to voices found in original audio. I tried to use different speaker embedding for each example including male and female voices with different properties, but the result was not better than the single speaker mentioned above.\n",
    "\n",
    "#### 3.3 Expected text\n",
    "\n",
    "I cleaned expected text by applying the following processing: lowercase, remove outer spaces, remove all characters except letters and spaces.\n",
    "\n",
    "#### 3.4 Transcription (Speech-to-Text)\n",
    "\n",
    "Given that positive labels in our dataset indicate that provided expected text matches the speech in the audio file, the natural idea is to finetune the transcription model on these examples. There are about 18k positive examples which is large enough. Finetuning gives us two advantages: we can obtain transcription of the audio and use it in addition to the expected text, and also finetuning improves the encoder part of the Whisper model, which we will use later in multimodal classifier. \n",
    "\n",
    "I created a 5-fold cross-validation split to be able to predict the full training set using 5 out-of-fold parts. I used the `WhisperForConditionalGeneration` wrapper, `AdamW` optimizer and `CrossEntropyLoss` loss. I trained 4 epochs with a learning rate of `1e-5`. Best models were selected based on the loss, but I also computed WER and CER for evaluation purposes (see tables below).\n",
    "\n",
    "#### 3.5 Multimodal classifier\n",
    "\n",
    "When all data components are ready, we process them with a multimodal classifier. I just concatenated original audio with generated audio, and original text with generated text separated by a single space. Multimodal classifier uses encoder from the finetuned Whisper model as audio feature extractor and Deberta-v3 as text feature extractor. To obtain audio features I extracted the last hidden state from the Whisper encoder and computed the average. To obtain text features I extracted the first token (CLS) from the last hidden state of Deberta. Extracted audio and text features are concatenated and processed by a fully connected fusion layer. Dimensions of audio features are from 768 to 1280 (depending on Whisper size) and dimension of text features is 768. As a fusion layer I chose a fully connected layer of size 256. I use ReLU as a fusion layer activation. The final layer has dimension 2 and outputs final logits.\n",
    "\n",
    "I trained a multimodal classifier using the same exact 5-fold split which was used for transcription tuning to avoid data leakage. Only the first fold (index 0) is used in the final solution. I trained each model using 7 sets of 2 epochs each with `Adam` optimizer and `1e-5` learning rate. I.e. I just rerun the same training command 7 times and selected the model with the best validation score. The reason for such an approach is that the model demonstrates fast convergence (only 2 epochs) with moderate variation in the validation scores. I tried lower learning rates and different schedulers (e.g. cosine annealing) but eventually the rerun-and-select strategy turned out to be better. When using this approach it is possible to develop a slight overfitting to the validation set, but in my experiments better local scores were still well correlated with leaderboard scores.\n",
    "\n",
    "#### 3.6 Ensemble\n",
    "\n",
    "For the ensemble I used 6 models (mentioned at the beginning) and computed the average of predicted probabilities.\n",
    "\n",
    "### 4. Do you have any useful charts, graphs, or visualizations from the process?\n",
    "\n",
    "I concentrated on numerical optimization and used visuals only a couple times to look at Whisper spectrograms. So I don’t have useful graphs.\n",
    "\n",
    "\n",
    "### 5. Copy and paste the 3 most impactful parts of your code and explain what each does and how it helped your model.\n",
    "\n",
    "1) The most impactful part of my code is the multimodal classifier. It extracts audio and text features and then processes them jointly to classify an example. I provide a definition of the forward pass.\n",
    "\n",
    "```python\n",
    "def forward(self, images, input_ids, attention_mask):\n",
    "    # Extract audio features\n",
    "    image_features = self.image_feature_extractor(images).last_hidden_state\n",
    "    image_features = image_features.mean(dim=1)\n",
    "    # Extract text features\n",
    "    text_outputs = self.text_feature_extractor(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    text_features = text_outputs.last_hidden_state[:, 0, :]\n",
    "    # Fuse\n",
    "    combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "    combined_features = self.fc_fusion(combined_features)\n",
    "    combined_features = self.relu(combined_features)\n",
    "    # Cls\n",
    "    output = self.classifier(combined_features)\n",
    "    return output\n",
    "```\n",
    "\n",
    "2) Also I cleaned all expected text and generated transcriptions using the following code. Lowercase, trim outer spaces, leave only letters and spaces.\n",
    "\n",
    "```python\n",
    "def clean(x):\n",
    "    return re.sub('[^a-zA-Z ]+', '', x.strip().lower())\n",
    "```\n",
    "\n",
    "### 6. Please provide the machine specs and time you used to run your model.\n",
    "\n",
    "Hardware:\n",
    "\n",
    "* 12x CPU\n",
    "* 32 GB RAM\n",
    "* 1x RTX-3090-24GB GPU\n",
    "* 500 GB SSD\n",
    "\n",
    "Software\n",
    "\n",
    "* Ubuntu 22.04\n",
    "* Python: 3.10.12 (Conda)\n",
    "* CUDA 12.4\n",
    "\n",
    "Time:\n",
    "\n",
    "* Training time: **140 hours**  \n",
    "* Inference time: **1 hour**\n",
    "\n",
    "**Note 1:** It's not possible to train 2 of 6 models used in my solution on 16 GB GPU. Specifically, `distil-whisper/distil-large-v3` and `distil-whisper/distil-large-v2` cannot be trained on `T4-16GB` and `V100-16GB` even with batch size 1 and mixed precision.\n",
    "\n",
    "**Note 2:** I successfully tested the solution on the following GPUs: `RTX-3090-24GB`, `L4-24GB`, `A100-40GB`.\n",
    "\n",
    "**Note 3:** I provide bash scripts with a suffix `_a100` adapted for training on `A100-40GB` GPU which can speed up the training by about 2 times.\n",
    "\n",
    "### 7. Anything we should watch out for or be aware of in using your model (e.g. code quirks, memory requirements, numerical stability issues, etc.)?\n",
    "\n",
    "1) It's not possible to train 2 of the 6 models used in my solution on a 16 GB GPU. Please see section 6 above for details.\n",
    "\n",
    "2) There is a well known warning from Huggingface tokenizers: `The current process just got forked …` related to possible excessive parallelism. I mitigate this warning by explicitly allowing parallelism: `os.environ['TOKENIZERS_PARALLELISM'] = 'true'`. I did not have any issues with this setting, but just in case of any problems it is possible to replace `true` with `false` at the beginning of the scripts to get more conservative behaviour.\n",
    "\n",
    "3) For many complicated examples with unclear speech, especially for nonword repetition task, Whisper tends to hallucinate. For example if expected text is `koovnorb` the audio transcription may take the following forms:  \n",
    "a) repetition of the expected text: `koovnorbkoovnorbkoovnorb`  \n",
    "b) random letters after the expected text: `koovnorbteevohkmerforbuhkahydohfteevohk`  \n",
    "I did not apply any special means to mitigate hallucination because the Deberta model, which processes concatenated expected text and transcription, can easily identify useful information i.e. the beginning of the transcription. \n",
    "\n",
    "4) Within my code in variable names and comments I use the terms `image` and `audio` interchangeably in relation to audio data. And in contrast I use the term `text` to denote textual information.\n",
    "\n",
    "5) All relevant scripts accept parameters `batch_size` and `accum`, where `accum` means number of steps for gradient accumulation. Please note, that it is possible to adjust this parameters based on target hardware, but resulting batch size (i.e. product of  `batch_size` * `accum`) should stay the same. Specifically, to finetune the transcription model (script `train_trans.py`) I used the resulting batch size 36 (e.g. `batch_size=4, accum=9`). To train a multimodal classifier (script `train_cls.py`) I used resulting batch size 32 (e.g. `batch_size=8, accum=4`).\n",
    "\n",
    "\n",
    "### 8. Did you use any tools for data preparation or exploratory data analysis that aren’t listed in your code submission?\n",
    "\n",
    "No. I did not use any additional tools. At the exploratory data analysis stage I mostly listened to the provided audio to develop some cleaning or preprocessing methods. Eventually I decided to use original audio unchanged, because Whisper was trained with very large and versatile data.\n",
    "\n",
    "\n",
    "### 9. How did you evaluate performance of the model other than the provided metric, if at all?\n",
    "\n",
    "For local evaluation I used the same stratified 5-fold split for both transcription model training and multimodal classifier training and used out-of-fold predictions to compute metrics. For the transcription in addition to the cross-entropy loss I computed WER and CER metrics. For the multimodal classifier I calculated accuracy in addition to the log loss. Local log loss scores were very close (slightly better) and consistent with leaderboard scores. Accuracy closely followed changes in log loss. Please see `Table 1` and `Table 2` for all metric results.\n",
    "\n",
    "\n",
    "### 10. What are some other things you tried that didn’t necessarily make it into the final workflow (quick overview)?\n",
    "\n",
    "My best multimodal classifier uses Whisper encoder as audio feature extractor and Deberta-v3 as text feature extractor. I tried the following modifications without improvement:\n",
    "\n",
    "1) Add another audio feature extractor (`Wav2Vec2`)\n",
    "2) Add image model (`EfficientNet-B7`) as audio feature extractor based on mel-spectrograms\n",
    "3) Replace text feature extractor with: `mDeberta` (multilingual), `Bert`, `Roberta`, `XLM-Roberta` (multilingual), `SentenceTransformer`.\n",
    "4) Given that Whispr accepts a fixed sampling rate of 16 kHz, I tried to resample audio into other sampling rates which effectively makes speech slower or faster: 8 kHz, 24 kHz, 32 hHz.\n",
    "5) In the final submission I generated speech from expected text using a single female voice (i.e. the same speaker embedding), but I also tried to generate a different voice for each example without improvement.\n",
    "\n",
    "\n",
    "### 11. If you were to continue working on this problem for the next year, what methods or techniques might you try in order to build on your work so far? Are there other fields or features you felt would have been very helpful to have?\n",
    "\n",
    "To continue work on this problem first of all I would try more different feature extractors for audio. Also I would be interested in researching features extracted using signal processing techniques.\n",
    "\n",
    "### 12. What simplifications could be made to run your solution faster without sacrificing significant accuracy?\n",
    "\n",
    "Easy simplification of my solution is to use only the single best model (based on `whisper-medium.en` or `whisper-medium`) instead of the all 6 models. Given that the best model is not the largest, training time may be reduced by almost 10 times with very little log loss degradation around 0.25 vs 0.21. I provide adaptation of my solution for a single model via scripts with the `_single` suffix. Please see the last section of the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f2cfb-bd9a-4b1c-8cfc-3566b9846d13",
   "metadata": {},
   "source": [
    "# B) Results and conclusions\n",
    "\n",
    "***Table 1. Tuning transcription model on positive examples.***  \n",
    "Input to the model is original audio, ground truth is cleaned expected text.  \n",
    "All values are averages of 5 scores (from each of 5 folds).  \n",
    "CE loss (cross-entropy loss), WER (word error rate), CER (character error rate).  \n",
    "\n",
    "| Model                             | CE loss       | WER           | CER           |\n",
    "|---------------------------------- |---------------|---------------|---------------|\n",
    "| `openai/whisper-medium.en`        | 0.0840        | 0.2245        | 0.6490        |\n",
    "| `openai/whisper-medium`           | **0.0779**    | 0.2099        | 0.5800        |\n",
    "| `openai/whisper-small.en`         | 0.0932        | **0.1866**    | **0.2962**    |\n",
    "| `distil-whisper/distil-medium.en` | 0.1239        | 0.4258        | 1.3495        |\n",
    "| `distil-whisper/distil-large-v3`  | 0.1310        | 0.2406        | 0.8515        |\n",
    "| `distil-whisper/distil-large-v2`  | 0.1072        | 0.3424        | 1.0165        |\n",
    "\n",
    "\n",
    "***Table 2. Training multimodal classifier.***  \n",
    "Input to the model is: original audio, generated audio, expected text, transcription.  \n",
    "Ground truth is target score (0 or 1).  \n",
    "Values in columns `Log loss` and `Accuracy` are averages of 5 scores (from each of 5 folds).  \n",
    "Values in columns `LB Public` and `LB Private` are leaderboard results from submitting ensemble of 5 folds of the corresponding model.  \n",
    "[1] Audio encoder from `whisper-medium.en`, transcriptions from `whisper-medium.en`  \n",
    "[2] Audio encoder from `whisper-medium.en`, transcriptions from `whisper-small.en`  \n",
    "[3] Audio encoder from `distil-large-v3`, transcriptions from `distil-large-v3`  \n",
    "[4] Audio encoder from `distil-large-v3`, transcriptions from `whisper-medium.en`  \n",
    "Models [2] and [3] were not used in the final ensemble.\n",
    "\n",
    "| Model                                | Log loss      | Accuracy      | LB Public  | LB Private  |\n",
    "|--------------------------------------|---------------|---------------|------------|-------------|\n",
    "| `openai/whisper-medium.en` [1]       | 0.2441        | 0.9032        | 0.2218     | 0.2203      |\n",
    "| `openai/whisper-medium.en` [2]       | 0.2418        | 0.9037        | 0.2313     | 0.2294      |\n",
    "| `openai/whisper-medium`              | **0.2358**    | **0.9047**    |            |             |\n",
    "| `openai/whisper-small.en`            | 0.2542        | 0.8978        |            |             |\n",
    "| `distil-whisper/distil-medium.en`    | 0.2481        | 0.8998        |            |             |\n",
    "| `distil-whisper/distil-large-v3`[3]  | 0.2437        | 0.9019        |            |             |\n",
    "| `distil-whisper/distil-large-v3`[4]  | 0.2401        | 0.9036        |            |             |\n",
    "| `distil-whisper/distil-large-v2`     | 0.2463        | 0.8988        |            |             |\n",
    "|                                      |               |               |            |             |\n",
    "| `ensemble`                           |               |               | **0.2156** | **0.2137**  |\n",
    "\n",
    "\n",
    "#### Conclusions:\n",
    "\n",
    "1) The best single multimodal classifiers are `openai/whisper-medium.en` and `openai/whisper-medium` (`Table 2`). Best local loss belongs to `openai/whisper-medium`, but I did not submit it, so there is no LB score to confirm its dominance. Also during my experimentation I found that English specific models (`.en`) tend to perform better than multilingual models (at least more stable).\n",
    "2) The best transcription models are again `openai/whisper-medium.en` and `openai/whisper-medium` (`Table 1`).\n",
    "3) Transcription model `openai/whisper-small.en` looks like an interesting outlier given its low WER and CER scores (`0.1866` and `0.2962`, `Table 1`). Despite good transcription scores, the multimodal classifier based on this architecture wasn't very good (loss `0.2542`, `Table 2`). To investigate further I trained the `openai/whisper-medium.en` classifier using transcriptions from `openai/whisper-small.en`. Loss slightly improved (`0.2441` -> `0.2418`) but LB degraded (`0.2218` -> `0.2313`). Compare [1] and [2] in the `Table 2`.\n",
    "4) I ran another experiment with transcription replacement ([3] and [4]). Multimodal classifier based on `distil-whisper/distil-large-v3` was trained on native transcriptions and transcriptions from `openai/whisper-medium.en`. This replacement also allowed to obtain a slight improvement in local validation, but LB score is not available. In general results from native and borrowed transcriptions are close.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd35e1-bbd4-4f56-a143-e8430cd7238c",
   "metadata": {},
   "source": [
    "# C) Installation\n",
    "\n",
    "## Directory structure\n",
    "\n",
    "Directories `assets`, `data`, and `train` are not provided in the main solution archive. Please create them according to the instructions below.\n",
    "\n",
    "1) `assets` directory contains model weights and other artifacts used in the best ensemble. Please download the archive `assets_literacy.zip` from Google Drive [https://drive.google.com/file/d/1yTblWYRWD2Gu6y99dbcPGl5rL1SaU7AP/view?usp=sharing](https://drive.google.com/file/d/1yTblWYRWD2Gu6y99dbcPGl5rL1SaU7AP/view?usp=sharing) and extract it `unzip assets_literacy.zip`. Size of the archive is about 19 GB. It does  not contain code and is needed only to replicate inference. No need to download for retraining from scratch.  \n",
    "2) `data` directory contains test dataset (`.wav` and `.csv` files, like on the remote evaluation server). Please create it from the corresponding competition or holdout data.\n",
    "3) `train` directory contains training dataset (`.wav` and `.csv` files). Please create it from the corresponding competition or holdout data.\n",
    "\n",
    "```\n",
    "solution\n",
    "|\n",
    "|-- assets\n",
    "|   |-- cls0-mediumen-f0-e001-0.2331-acc-0.9069.bin\n",
    "|   |-- ...\n",
    "|\n",
    "|-- data\n",
    "|   |-- bfaiol.wav\n",
    "|   |-- czfqjg.wav\n",
    "|   |-- ...\n",
    "|   |-- submission_format.csv\n",
    "|   |-- test_labels.csv\n",
    "|   |-- test_metadata.csv\n",
    "|\n",
    "|-- train\n",
    "|   |-- aaabdo.wav\n",
    "|   |-- aaacth.wav\n",
    "|   |-- ...\n",
    "|   |-- train_labels.csv\n",
    "|   |-- train_metadata.csv\n",
    "|\n",
    "|-- infer_cls.py\n",
    "|-- ...\n",
    "|-- utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108da48-12aa-4663-9c20-3028b9b8117e",
   "metadata": {},
   "source": [
    "## Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5444ca5-e719-4e40-a851-1a2fc4900542",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda488d4-8e09-4c22-9ff4-3675ed3bc9dc",
   "metadata": {},
   "source": [
    "# D) Full solution  \n",
    "\n",
    "## 1. Inference using ready weights\n",
    "\n",
    "All commands below expect 24GB GPU.  \n",
    "To infer using the `T4-16GB` GPU, please set batch size 16 which was used on the remote eval server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8f7bc-d529-411f-a09f-08e8ca210a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer_cls.py \\\n",
    "--input_dir=data \\\n",
    "--assets_dir=assets \\\n",
    "--submission_file=submission.csv \\\n",
    "--batch_size=64 \\\n",
    "--device=cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2372a940-623d-436a-8ce9-525b90820d02",
   "metadata": {},
   "source": [
    "## 2. Training and inference from scratch\n",
    "\n",
    "All commands below expect 24GB GPU.  \n",
    "To run on `A100-40GB` GPU with adapted batch sizes please use corresponding commented out scripts with a suffix `_a100` instead of original ones.  \n",
    "In case of modifying `batch_size` parameter for training scripts please change `accum` parameter accordingly. I.e. the product of these two parameters should stay the same: `36` for transcription model, and `32` for multimodal classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94fad72-a5ea-45d5-a0c2-bfefb33eecdf",
   "metadata": {},
   "source": [
    "#### Generate speech\n",
    "\n",
    "Generate speech for each unique expected text in the training set and save as pickled dict in `output_dir`.  \n",
    "Keys are unique sentences of expected text, and values are numpy arrays of waveform.  \n",
    "During inference we will reuse this dictionary. We will generate speech only for those expected texts which are not found in this dict.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9cfba2-a673-4a8c-8234-716fe503ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer_speech.py \\\n",
    "--input_dir=train \\\n",
    "--output_dir=assets_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a2790-ae32-43b5-8510-819521b96534",
   "metadata": {},
   "source": [
    "#### Train transcription models\n",
    "\n",
    "Train 6 transcription models with 5 folds each.  \n",
    "**Note.** Inside `train_trans.sh` we call script `train_trans.py` independently for each of 5 folds while the script itself allows to train 5 folds in a loop. This approach allows us to use GPU vRAM more efficiently i.e. to use larger batches. Exiting the `.py` script after each fold allows to completely free GPU vRAM which otherwise is not possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea55b8-f850-4d7b-b5aa-12894520302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash train_trans.sh\n",
    "# !bash train_trans_a100.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447d01b-e255-4e2c-99cc-6fb20bce6d63",
   "metadata": {},
   "source": [
    "#### Infer transcription models\n",
    "\n",
    "Infer transcriptions for the whole training set using 5 out-of-fold parts.  \n",
    "We run only 5 models out of 6 because the classifier based on `distil-whisper/distil-large-v3` encoder will use transcription form `openai/whisper-medium.en`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b162c97b-8c8b-4c05-840a-f0f3636aaa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash infer_trans.sh\n",
    "# !bash infer_trans_a100.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f123155-4479-4a93-91c9-029790d8277b",
   "metadata": {},
   "source": [
    "#### Train classification models\n",
    "\n",
    "Train each of 6 classifier models 7 times. Such an approach allows to obtain better validation scores than learning rate schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ff950-4fbd-4aad-ac0e-86d66f75c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash train_cls.sh\n",
    "# !bash train_cls_a100.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f5c4c-6a51-44c6-a944-fefb90043438",
   "metadata": {},
   "source": [
    "#### Select best weights and copy them to a new assets directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc6fa6-8e5e-4e0d-82fa-5bf88d44f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python select_weights.py \\\n",
    "--input_dir=./ \\\n",
    "--assets_dir=assets_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8987a269-7f4a-4a7c-a9b9-ae4639d099e7",
   "metadata": {},
   "source": [
    "#### Infer test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5645e082-45cc-4c50-a819-1f9bd3ee20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer_cls.py \\\n",
    "--input_dir=data \\\n",
    "--assets_dir=assets_new \\\n",
    "--submission_file=submission_new.csv \\\n",
    "--batch_size=64 \\\n",
    "--device=cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10457740-a1eb-4568-be6c-fa3be18ccabb",
   "metadata": {},
   "source": [
    "# F) Simplified solution (single model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f998924-c423-445a-a8b9-98f98fed1d86",
   "metadata": {},
   "source": [
    "## 1. Simplified inference using ready weights\n",
    "\n",
    "**Note.** Please extract the solution package archive again before running to avoid any artifact conflicts. Commands below will reuse intermediate directory and asset names which were used while running the full solution above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310cabba-a7c4-433a-a7e8-2292a7abb937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer_cls_single.py \\\n",
    "--input_dir=data \\\n",
    "--assets_dir=assets \\\n",
    "--submission_file=submission_single.csv \\\n",
    "--batch_size=64 \\\n",
    "--device=cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd7414-e6a4-41b1-8b49-55dadd895639",
   "metadata": {},
   "source": [
    "## 2. Simplified training and inference from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7731162a-046f-4bd2-b17e-d5892b00546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer_speech.py \\\n",
    "--input_dir=train \\\n",
    "--output_dir=assets_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c66ce2-c6e0-49b7-91cf-eeb683efaad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash train_trans_single.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05bb6d1-b3fd-4d74-9cca-effc93d2bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash infer_trans_single.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472baba-7193-488a-a106-c6c256a30f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash train_cls_single.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227f067-e205-4c15-b484-55d9ea672a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python select_weights_single.py \\\n",
    "--input_dir=./ \\\n",
    "--assets_dir=assets_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd2ce6-c2fe-44ed-b7e1-6bfdce972e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer_cls_single.py \\\n",
    "--input_dir=data \\\n",
    "--assets_dir=assets_new \\\n",
    "--submission_file=submission_single_new.csv \\\n",
    "--batch_size=64 \\\n",
    "--device=cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3efa9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
